{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# The Task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Project Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "In the file dataset/data.csv, you will find a dataset containing news articles with the following columns:\n",
    "\n",
    "label: 0 if the news is fake, 1 if the news is real.\n",
    "title: The headline of the news article.\n",
    "text: The full content of the article.\n",
    "subject: The category or topic of the news.\n",
    "date: The publication date of the article.\n",
    "Your goal is to build a classifier that is able to distinguish between the two.\n",
    "\n",
    "Once you have a classifier built, then use it to predict the labels for dataset/validation_data.csv. Generate a new file where the label 2 has been replaced by 0 (fake) or 1 (real) according to your model. Please respect the original file format, do not include extra columns, and respect the column separator.\n",
    "\n",
    "Please ensure to split the data.csv into training and test datasets before using it for model training or evaluation.\n",
    "\n",
    "Guidance\n",
    "Like in a real life scenario, you are able to make your own choices and text treatment. Use the techniques you have learned and the common packages to process this data and classify the text.\n",
    "\n",
    "Deliverables\n",
    "Python Code: Provide well-documented Python code that conducts the analysis.\n",
    "Predictions: A csv file in the same format as validation_data.csv but with the predicted labels (0 or 1)\n",
    "Accuracy estimation: Provide the teacher with your estimation of how your model will perform.\n",
    "Presentation: You will present your model in a 10-minute presentation. Your teacher will provide further instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "from nltk import pos_tag\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "# Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('dataset/training_data_lowercase.csv', sep='\\t', names=['labels', 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "# Understanding the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Its unique values are ',data['labels'].unique())\n",
    "print(print(data['labels'].describe()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(data.labels, color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "# Preprocessing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## Basic cleaning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_html_text(text: str) -> str:\n",
    "    if text is None:\n",
    "        return ''\n",
    "    text = str(text)\n",
    "    # Remove inline JavaScript/CSS\n",
    "    text = re.sub(r\"(?is)<script.*?>.*?</script>\", \" \", text)\n",
    "    text = re.sub(r\"(?is)<style.*?>.*?</style>\", \" \", text)\n",
    "    # Remove HTML comments\n",
    "    text = re.sub(r\"(?s)<!--.*?-->\", \" \", text)\n",
    "    # Remove the remaining tag\n",
    "    text = re.sub(r\"(?s)<[^>]+>\", \" \", text)\n",
    "    # Remove prefixed b\n",
    "    text = re.sub(r\"^\\s*b[\\\"'](.+?)[\\\"']\\s*$\", r\"\\1\", text)\n",
    "    # Remove end of the line tags\n",
    "    text = re.sub(r\"\\s*[\\[\\(][^\\]\\)]+[\\]\\)]\\s*$\", \"\", text)    \n",
    "    # Remove \\t from middle and end of the texts\n",
    "    text = re.sub(r\"\\b\\\\t\",\" \",text)\n",
    "    # Remove \\t from start of the texts\n",
    "    text = re.sub(r\"^\\\\t\",\" \",text)\n",
    "    # Remove all the special characters and numbers\n",
    "    text = re.sub(r\"[^A-Za-z\\s]\", \" \", text)\n",
    "    # Remove all single characters\n",
    "    text = re.sub(r\"\\b[A-Za-z]\\b\", \" \", text)\n",
    "    # Remove single characters from the start\n",
    "    text = re.sub(r\"^[A-Za-z]\\s+\", \" \", text)\n",
    "    # Substitute multiple spaces with single space\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "punct_pattern = f\"[{re.escape(string.punctuation)}]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['pre_text'] = data['text'].astype(str).apply(lambda x: clean_html_text(x))\n",
    "data['pre_text'] = data['pre_text'].astype(str).apply(lambda x: re.sub(punct_pattern, \"\", x))\n",
    "data['pre_text'] = data['pre_text'].astype(str).apply(lambda x: word_tokenize(x))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "## Removing stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['pre_text'] = data['pre_text'].apply(lambda tokens: [word for word in tokens if word not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words = {}\n",
    "\n",
    "for lista in data['pre_text']:\n",
    "    for word in lista:\n",
    "        if bag_of_words == 0:\n",
    "            bag_of_words[word] = 1\n",
    "        elif word in bag_of_words:\n",
    "            bag_of_words[word] +=1\n",
    "        else:\n",
    "            bag_of_words[word] = 1\n",
    "\n",
    "print(sorted(bag_of_words.items(), key=lambda x: -x[1])[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_filter = ['video','says', 'tweets', 'tells','screenshots',\n",
    "                   'details', 'fck', 'btch', 'images', 'cck', 'image'\n",
    "                   ,'videos','ahole']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['pre_text_filter'] = data['pre_text'].apply(lambda tokens: [word for word in tokens if word not in words_to_filter])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words = {}\n",
    "\n",
    "for lista in data['pre_text_filter']:\n",
    "    for word in lista:\n",
    "        if bag_of_words == 0:\n",
    "            bag_of_words[word] = 1\n",
    "        elif word in bag_of_words:\n",
    "            bag_of_words[word] +=1\n",
    "        else:\n",
    "            bag_of_words[word] = 1\n",
    "\n",
    "print(sorted(bag_of_words.items(), key=lambda x: -x[1])[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "### Using Stemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "#### Snowball"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "snowball = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['snow_text'] = data['pre_text'].apply(lambda tokens: [snowball.stem(token) for token in tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "#### Porter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "porter = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['porter_text'] = data['pre_text'].apply(lambda tokens: [porter.stem(token) for token in tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "### Using Lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemm = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['lemm_text'] = data['pre_text'].apply(lambda tokens: [lemm.lemmatize(token) for token in tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "### Using Lemmatizer (Verb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['lemm_text_verb'] = data['pre_text'].apply(lambda tokens: [lemm.lemmatize(token, pos='v') for token in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Spliting the data into Training and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.iloc[:,2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data.iloc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "## Using only the preprocessed text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X['pre_text'], y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {},
   "source": [
    "## Using the preprocessed text + snow stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_snow, X_test_snow, y_train_snow, y_test_snow = train_test_split(X['snow_text'], y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {},
   "source": [
    "## Using the preprocessed text + porter stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_porter, X_test_porter, y_train_porter, y_test_porter = train_test_split(X['porter_text'], y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {},
   "source": [
    "## Using the preprocessed text + noise removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_filt, X_test_filt, y_train_filt, y_test_filt = train_test_split(X['pre_text_filter'], y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50",
   "metadata": {},
   "source": [
    "## Using the preprocessed text + lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_lemm, X_test_lemm, y_train_lemm, y_test_lemm = train_test_split(X['lemm_text'], y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52",
   "metadata": {},
   "source": [
    "## Using the preprocessed text + lemmatizer (Verbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_lemm_verb, X_test_lemm_verb, y_train_lemm_verb, y_test_lemm_verb = train_test_split(X['lemm_text_verb'], y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54",
   "metadata": {},
   "source": [
    "### Defining a plotting function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "def plot_learning_curve(\n",
    "    model,\n",
    "    X,\n",
    "    y,\n",
    "    scoring=\"accuracy\",\n",
    "    cv=5,\n",
    "    train_sizes=np.linspace(0.1, 1.0, 5),\n",
    "    title=None\n",
    "):\n",
    "    train_sizes, train_scores, val_scores = learning_curve(\n",
    "        model,\n",
    "        X,\n",
    "        y,\n",
    "        scoring=scoring,\n",
    "        cv=cv,\n",
    "        train_sizes=train_sizes,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    train_mean = train_scores.mean(axis=1)\n",
    "    val_mean = val_scores.mean(axis=1)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(train_sizes, train_mean, marker=\"o\", label=\"Training score\")\n",
    "    plt.plot(train_sizes, val_mean, marker=\"o\", label=\"Validation score\")\n",
    "    plt.xlabel(\"Training set size\")\n",
    "    plt.ylabel(scoring)\n",
    "    plt.title(title or model.__class__.__name__)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56",
   "metadata": {},
   "source": [
    "# Training some classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58",
   "metadata": {},
   "source": [
    "### Only preprocessed text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59",
   "metadata": {},
   "source": [
    "#### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(\n",
    "    tokenizer=lambda x: x,\n",
    "    preprocessor=lambda x: x,\n",
    "    token_pattern=None,\n",
    "    lowercase=False\n",
    ")\n",
    "\n",
    "X_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61",
   "metadata": {},
   "source": [
    "##### Decision Tree metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_classifier = DecisionTreeClassifier(max_depth=10, random_state=42).fit(X_tfidf, y_train)\n",
    "y_hat = dt_classifier.predict(X_test_tfidf)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_hat))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_hat))\n",
    "print('Confusion matrix:\\n', confusion_matrix(y_test, y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curve(\n",
    "    model = dt_classifier,\n",
    "    X = X_tfidf,\n",
    "    y = y_train,\n",
    "    scoring=\"f1\",\n",
    "    title=\"DecisionTreeClassifier - TF-IDF\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64",
   "metadata": {},
   "source": [
    "#### BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(\n",
    "    tokenizer=lambda x: x,\n",
    "    preprocessor=lambda x: x,\n",
    "    token_pattern=None,\n",
    "    lowercase=False\n",
    ")\n",
    "\n",
    "X_bow = vectorizer.fit_transform(X_train)\n",
    "X_test_bow = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66",
   "metadata": {},
   "source": [
    "##### Decision Tree metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_classifier = DecisionTreeClassifier(max_depth=10, random_state=42).fit(X_bow, y_train)\n",
    "y_hat = dt_classifier.predict(X_test_bow)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_hat))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_hat))\n",
    "print('Confusion matrix:\\n', confusion_matrix(y_test, y_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68",
   "metadata": {},
   "source": [
    "#### Plotting Learning Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curve(\n",
    "    model = dt_classifier,\n",
    "    X = X_bow,\n",
    "    y = y_train,\n",
    "    scoring=\"f1\",\n",
    "    title=\"Passive Aggressive – TF-IDF\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70",
   "metadata": {},
   "source": [
    "### Preprocessed text + noise removal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71",
   "metadata": {},
   "source": [
    "#### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(\n",
    "    tokenizer=lambda x: x,\n",
    "    preprocessor=lambda x: x,\n",
    "    token_pattern=None,\n",
    "    lowercase=False\n",
    ")\n",
    "\n",
    "X_tfidf = vectorizer.fit_transform(X_train_filt)\n",
    "X_test_tfidf = vectorizer.transform(X_test_filt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73",
   "metadata": {},
   "source": [
    "##### Decision Tree metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_classifier = DecisionTreeClassifier(max_depth=10, random_state=42).fit(X_tfidf, y_train)\n",
    "y_hat = dt_classifier.predict(X_test_tfidf)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test_filt, y_hat))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test_filt, y_hat))\n",
    "print('Confusion matrix:\\n', confusion_matrix(y_test_filt, y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curve(\n",
    "    model = dt_classifier,\n",
    "    X = X_tfidf,\n",
    "    y = y_train,\n",
    "    scoring=\"f1\",\n",
    "    title=\"Passive Aggressive – TF-IDF\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76",
   "metadata": {},
   "source": [
    "#### BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(\n",
    "    tokenizer=lambda x: x,\n",
    "    preprocessor=lambda x: x,\n",
    "    token_pattern=None,\n",
    "    lowercase=False\n",
    ")\n",
    "\n",
    "X_bow = vectorizer.fit_transform(X_train_filt)\n",
    "X_test_bow = vectorizer.transform(X_test_filt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78",
   "metadata": {},
   "source": [
    "##### Decision Tree metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_classifier = DecisionTreeClassifier(max_depth=10, random_state=42).fit(X_bow, y_train_filt)\n",
    "y_hat = dt_classifier.predict(X_test_bow)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test_filt, y_hat))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test_filt, y_hat))\n",
    "print('Confusion matrix:\\n', confusion_matrix(y_test_filt, y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curve(\n",
    "    model = dt_classifier,\n",
    "    X = X_bow,\n",
    "    y = y_train_filt,\n",
    "    scoring=\"f1\",\n",
    "    title=\"Passive Aggressive – TF-IDF\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81",
   "metadata": {},
   "source": [
    "### Preprocessed text + snow stemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82",
   "metadata": {},
   "source": [
    "#### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(\n",
    "    tokenizer=lambda x: x,\n",
    "    preprocessor=lambda x: x,\n",
    "    token_pattern=None,\n",
    "    lowercase=False\n",
    ")\n",
    "\n",
    "X_tfidf = vectorizer.fit_transform(X_train_snow)\n",
    "X_test_tfidf = vectorizer.transform(X_test_snow)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84",
   "metadata": {},
   "source": [
    "##### Decision Tree metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_classifier = DecisionTreeClassifier(max_depth=10, random_state=42).fit(X_tfidf, y_train_snow)\n",
    "y_hat = dt_classifier.predict(X_test_tfidf)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test_snow, y_hat))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test_snow, y_hat))\n",
    "print('Confusion matrix:\\n', confusion_matrix(y_test_snow, y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curve(\n",
    "    model = dt_classifier,\n",
    "    X = X_tfidf,\n",
    "    y = y_train_snow,\n",
    "    scoring=\"f1\",\n",
    "    title=\"Passive Aggressive – TF-IDF\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87",
   "metadata": {},
   "source": [
    "#### BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(\n",
    "    tokenizer=lambda x: x,\n",
    "    preprocessor=lambda x: x,\n",
    "    token_pattern=None,\n",
    "    lowercase=False\n",
    ")\n",
    "\n",
    "X_bow = vectorizer.fit_transform(X_train_snow)\n",
    "X_test_bow = vectorizer.transform(X_test_snow)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89",
   "metadata": {},
   "source": [
    "##### Decision Tree metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_classifier = DecisionTreeClassifier(max_depth= 10, random_state=42).fit(X_bow, y_train_snow)\n",
    "y_hat = dt_classifier.predict(X_test_bow)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test_snow, y_hat))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test_snow, y_hat))\n",
    "print('Confusion matrix:\\n', confusion_matrix(y_test_snow, y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curve(\n",
    "    model = dt_classifier,\n",
    "    X = X_bow,\n",
    "    y = y_train_snow,\n",
    "    scoring=\"f1\",\n",
    "    title=\"Decision Tree Classifier – BoW\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92",
   "metadata": {},
   "source": [
    "### Preprocessed text + porter stemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93",
   "metadata": {},
   "source": [
    "#### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(\n",
    "    tokenizer=lambda x: x,\n",
    "    preprocessor=lambda x: x,\n",
    "    token_pattern=None,\n",
    "    lowercase=False\n",
    ")\n",
    "\n",
    "X_tfidf = vectorizer.fit_transform(X_train_porter)\n",
    "X_test_tfidf = vectorizer.transform(X_test_porter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95",
   "metadata": {},
   "source": [
    "##### Decision Tree metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_classifier = DecisionTreeClassifier(max_depth=10, random_state=42).fit(X_tfidf, y_train_porter)\n",
    "y_hat = dt_classifier.predict(X_test_tfidf)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test_porter, y_hat))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test_porter, y_hat))\n",
    "print('Confusion matrix:\\n', confusion_matrix(y_test_porter, y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curve(\n",
    "    model = dt_classifier,\n",
    "    X = X_tfidf,\n",
    "    y = y_train_porter,\n",
    "    scoring=\"f1\",\n",
    "    title=\"Decision Tree Classifier – TF-IDF\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98",
   "metadata": {},
   "source": [
    "#### BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(\n",
    "    tokenizer=lambda x: x,\n",
    "    preprocessor=lambda x: x,\n",
    "    token_pattern=None,\n",
    "    lowercase=False\n",
    ")\n",
    "\n",
    "X_bow = vectorizer.fit_transform(X_train_porter)\n",
    "X_test_bow = vectorizer.transform(X_test_porter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100",
   "metadata": {},
   "source": [
    "##### Decision Tree metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_classifier = DecisionTreeClassifier(max_depth=10, random_state=42).fit(X_bow, y_train_porter)\n",
    "y_hat = dt_classifier.predict(X_test_bow)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test_porter, y_hat))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test_porter, y_hat))\n",
    "print('Confusion matrix:\\n', confusion_matrix(y_test_porter, y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curve(\n",
    "    model = dt_classifier,\n",
    "    X = X_bow,\n",
    "    y = y_train_porter,\n",
    "    scoring=\"f1\",\n",
    "    title=\"Decision Tree Classifier – BoW\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103",
   "metadata": {},
   "source": [
    "### Preprocessed text + porter lemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104",
   "metadata": {},
   "source": [
    "#### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(\n",
    "    tokenizer=lambda x: x,\n",
    "    preprocessor=lambda x: x,\n",
    "    token_pattern=None,\n",
    "    lowercase=False\n",
    ")\n",
    "\n",
    "X_tfidf = vectorizer.fit_transform(X_train_porter)\n",
    "X_test_tfidf = vectorizer.transform(X_test_porter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106",
   "metadata": {},
   "source": [
    "##### Decision Tree metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_classifier = DecisionTreeClassifier(max_depth=10, random_state=42).fit(X_tfidf, y_train_lemm)\n",
    "y_hat = dt_classifier.predict(X_test_tfidf)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test_lemm, y_hat))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test_lemm, y_hat))\n",
    "print('Confusion matrix:\\n', confusion_matrix(y_test_lemm, y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curve(\n",
    "    model = dt_classifier,\n",
    "    X = X_tfidf,\n",
    "    y = y_train_lemm,\n",
    "    scoring=\"f1\",\n",
    "    title=\"Decision Tree Classifier – TF-IDF\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109",
   "metadata": {},
   "source": [
    "#### BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(\n",
    "    tokenizer=lambda x: x,\n",
    "    preprocessor=lambda x: x,\n",
    "    token_pattern=None,\n",
    "    lowercase=False\n",
    ")\n",
    "\n",
    "X_bow = vectorizer.fit_transform(X_train_lemm)\n",
    "X_test_bow = vectorizer.transform(X_test_lemm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111",
   "metadata": {},
   "source": [
    "##### Decision Tree metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_classifier = DecisionTreeClassifier(max_depth=10, random_state=42).fit(X_bow, y_train_lemm)\n",
    "y_hat = dt_classifier.predict(X_test_bow)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test_lemm, y_hat))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test_lemm, y_hat))\n",
    "print('Confusion matrix:\\n', confusion_matrix(y_test_lemm, y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curve(\n",
    "    model = dt_classifier,\n",
    "    X = X_bow,\n",
    "    y = y_train_lemm,\n",
    "    scoring=\"f1\",\n",
    "    title=\"Decision Tree Classifier – BoW\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115",
   "metadata": {},
   "source": [
    "BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(\n",
    "    tokenizer=lambda x: x,\n",
    "    preprocessor=lambda x: x,\n",
    "    token_pattern=None,\n",
    "    lowercase=False\n",
    ")\n",
    "\n",
    "X_bow = vectorizer.fit_transform(X_train_lemm)\n",
    "X_test_bow = vectorizer.transform(X_test_lemm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression(random_state=42).fit(X_bow, y_train_lemm)\n",
    "y_hat = log_reg.predict(X_test_bow)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test_lemm, y_hat))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test_lemm, y_hat))\n",
    "print('Confusion matrix:\\n', confusion_matrix(y_test_lemm, y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curve(\n",
    "    model = log_reg,\n",
    "    X = X_bow,\n",
    "    y = y_train_lemm,\n",
    "    scoring=\"f1\",\n",
    "    title=\"Logistic Regression - BoW\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119",
   "metadata": {},
   "source": [
    "TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(\n",
    "    tokenizer=lambda x: x,\n",
    "    preprocessor=lambda x: x,\n",
    "    token_pattern=None,\n",
    "    lowercase=False\n",
    ")\n",
    "\n",
    "X_tfidf = vectorizer.fit_transform(X_train_lemm)\n",
    "X_test_tfidf = vectorizer.transform(X_test_lemm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression(random_state=42).fit(X_tfidf, y_train_lemm)\n",
    "y_hat = log_reg.predict(X_test_tfidf)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test_lemm, y_hat))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_hat))\n",
    "print('Confusion matrix:\\n', confusion_matrix(y_test_lemm, y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curve(\n",
    "    model = log_reg,\n",
    "    X = X_tfidf,\n",
    "    y = y_train_lemm,\n",
    "    scoring=\"f1\",\n",
    "    title=\"Logistic Regression - BoW\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123",
   "metadata": {},
   "source": [
    "Lemmatization (Verb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124",
   "metadata": {},
   "source": [
    "BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(\n",
    "    tokenizer=lambda x: x,\n",
    "    preprocessor=lambda x: x,\n",
    "    token_pattern=None,\n",
    "    lowercase=False\n",
    ")\n",
    "\n",
    "X_bow = vectorizer.fit_transform(X_train_lemm_verb)\n",
    "X_test_bow = vectorizer.transform(X_test_lemm_verb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression(random_state=42).fit(X_bow, y_train_lemm_verb)\n",
    "y_hat = log_reg.predict(X_test_bow)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test_lemm_verb, y_hat))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test_lemm_verb, y_hat))\n",
    "print('Confusion matrix:\\n', confusion_matrix(y_test_lemm_verb, y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curve(\n",
    "    model= log_reg,\n",
    "    X= X_bow,\n",
    "    y= y_train_lemm_verb,\n",
    "    scoring='f1',\n",
    "    title=\"LogisticRegression - BoW\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128",
   "metadata": {},
   "source": [
    "TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(\n",
    "    tokenizer=lambda x: x,\n",
    "    preprocessor=lambda x: x,\n",
    "    token_pattern=None,\n",
    "    lowercase=False\n",
    ")\n",
    "\n",
    "X_tfidf = vectorizer.fit_transform(X_train_lemm_verb)\n",
    "X_test_tfidf = vectorizer.transform(X_test_lemm_verb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression(random_state=42).fit(X_tfidf, y_train_lemm_verb)\n",
    "y_hat = log_reg.predict(X_test_tfidf)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test_lemm_verb, y_hat))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test_lemm_verb, y_hat))\n",
    "print('Confusion matrix:\\n', confusion_matrix(y_test_lemm_verb, y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curve(\n",
    "    model = log_reg,\n",
    "    X = X_tfidf,\n",
    "    y = y_train_lemm_verb,\n",
    "    scoring=\"f1\",\n",
    "    title=\"Logistic Regression - TF-IDF\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132",
   "metadata": {},
   "source": [
    "## Bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133",
   "metadata": {},
   "source": [
    "TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(\n",
    "    tokenizer=lambda x: x,\n",
    "    preprocessor=lambda x: x,\n",
    "    token_pattern=None,\n",
    "    lowercase=False,\n",
    "    # create word unigrams + bigrams\n",
    "    ngram_range=(1, 2),\n",
    "    # drops rare tokens/bigrams that appear once\n",
    "    min_df=2,\n",
    "    # drops near-constant boilerplate tokens\n",
    "    max_df=0.9\n",
    ")\n",
    "\n",
    "X_tfidf = vectorizer.fit_transform(X_train_porter)\n",
    "X_test_tfidf = vectorizer.transform(X_test_porter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression(random_state=42).fit(X_tfidf, y_train_lemm)\n",
    "y_hat = log_reg.predict(X_test_tfidf)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test_lemm, y_hat))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_hat))\n",
    "print('Confusion matrix:\\n', confusion_matrix(y_test_lemm, y_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136",
   "metadata": {},
   "source": [
    "Top Weighted Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = vectorizer.get_feature_names_out()\n",
    "coefs = log_reg.coef_[0]\n",
    "\n",
    "top_fake = sorted(zip(feature_names, coefs), key=lambda x: x[1], reverse=True)[:20]\n",
    "top_real = sorted(zip(feature_names, coefs), key=lambda x: x[1])[:20]\n",
    "\n",
    "top_real\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curve(\n",
    "    model = log_reg,\n",
    "    X = X_tfidf,\n",
    "    y = y_train_lemm,\n",
    "    scoring=\"f1\",\n",
    "    title=\"Logistic Regression - TF-IDF (Unigrams + Bigrams)\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
